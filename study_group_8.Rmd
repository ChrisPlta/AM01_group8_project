---
title: "Group8_Project"
author: "Misha Aggarwal, Madalina Dumitrescu, Yung-Chieh Hsu, Wendy Li, Christoph Plachutta, Tianyi Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(skimr)
library(janitor)
library(broom)
library(performance)
library(car)
library(lubridate)
```

```{r set_global_theme}

theme_set(theme_bw())

```

# Exploratory Data Analysis

## Inspecting and Cleaning the Data

As a first step to our EDA, we loud our data into the environment and assign it to variables. We use vroom, as it is superior in speed compared to read.csv.

```{r load_data}

# Load data into environment and assign to variables
sales <- vroom::vroom("data/sales.csv")
details <- vroom::vroom("data/details.csv")
stores <- vroom::vroom("data/stores.csv")

```

Next, we apply janitor::clean_names to bring the column names in order, to also make it easier joinable in the future.

```{r clean_names}

# Clean names of data frames
sales <- janitor::clean_names(sales)
details <- janitor::clean_names(details)
stores <- janitor::clean_names(stores)

```

By inspecting the data, we can see problems that may affect our future analysis. For each data frame, we change types for some of the variables.

```{r inspect_data_sales}

# Inspect Sales data frame
skim(sales)
glimpse(sales)
head(sales)

```
```{r clean_sales_data}

# Change store ID and dept to factor
sales <- sales %>% 
  mutate(store = as.factor(store),
         dept = as.factor(dept),
         # Change date format
         date = dmy(date))

```

```{r inspect_data_details}

# Inspect Sales data frame
skim(details)
glimpse(details)
head(details)

```

```{r clean_details_data}

# Change store ID and dept to factor
details <- details %>% 
  mutate(store = as.factor(store),
         # Change date format
         date = dmy(date),
         # Remove NAs in mark_downs
         mark_down1 = replace_na(mark_down1,0),
         mark_down2 = replace_na(mark_down2,0),
         mark_down3 = replace_na(mark_down3,0),
         mark_down4 = replace_na(mark_down4,0),
         mark_down5 = replace_na(mark_down5,0))

```

```{r inspect_data_stores}

# Inspect Sales data frame
skim(stores)
glimpse(stores)
head(stores)

```
```{r clean_stores_data}

# Change store ID and dept to factor
stores <- stores %>% 
  mutate(store = as.factor(store),
         type = as.factor(type))
```

As a last step, we join the dataframes together into one dataframe. As for the key, we can observe that all data frames share the "store" variable. For sales and details, we also have to include "date" and "is_holiday" in the join.

```{r join_dataframes}

# Join data frames together and assign to variable
joined_sales <- sales %>% 
  left_join(by = c("store","date","is_holiday"), y = details) %>% 
  left_join(by = "store", y = stores)

```

## Exploratory Analysis

We start by looking at whether the dates overlap for sales and details, so that we can assign the details values for each observation in the sales. As we can see from the below code and output, both dataframes start at the same date, while the details spans more weeks then the sales. This allows us to seamlessly join details to the sales.

```{r time_span_diff}

# Calculate weeks between min and max date for both data frames
sales %>% 
  summarize(max_date = max(date),
            min_date = min(date),
            weeks_covered = difftime(max_date, min_date, unit = "weeks"))

details %>% 
  summarize(max_date = max(date),
            min_date = min(date),
            weeks_covered = difftime(max_date, min_date, unit = "weeks"))

```

When investigating the size of the stores, we can also deduct that there are 3 stores with 79 distinct departments, namely 13, 15 and 19. This is the maximum amount of departments within a single store in the sample. Such a high number of departments leads us to make first assumptions about the type of store. We can perhaps the data to come from a company like Walmart, which are known to sell essentially everything in their larger stores, hence a lot of departments.

```{r max_departments}

# Calculate store with maximum departments  
sales %>% 
  group_by(store) %>% 
  summarize(count_distinct = n_distinct(dept)) %>%
  slice_max(count_distinct, n = 1)

```

To better understand the categorization of stores into their respective types, we investigate their correlated details. By graphing the size of the stores, colored by type, we can see that the size seems to be a significant indicator for a stores categorization. Nevertheless, we observe that some stores are very small, even though they are categorized in, for example, A.

```{r check_store_types}

# Graph size per store type
joined_sales %>% 
  filter(weekly_sales >= 0) %>% 
  mutate(store = fct_reorder(store,size)) %>% 
  ggplot(aes(x = store, y = size, fill = type)) +
        geom_col()

```
When trying to find the store with the most sales in 2011, we isolate store 4, which is indeed categorized as type A. It is also one of the larger stores within type A. Therefore we can already assume that there is a correlation between size of a store and sales, which we will investigate next.

```{r sales_overview_2011}

# Calculate store with most sales in 2011
sales %>% 
  # Select relevant time frame
  filter(date >= "2011-01-07" & date <= "2011-12-30") %>% 
  group_by(store) %>% 
  # Calculate summary statistics
  summarize(annl_sales = sum(weekly_sales),
            avg_wkly_sales = mean(weekly_sales)) %>% 
            slice_max(annl_sales, n = 1)

```

```{r distribution_weekly_sales}

# Create plot for distribution by store type
joined_sales %>% 
  # Remove days with negative weekly sales
  filter(weekly_sales >= 0) %>% 
  # Add log weekly_sales to account for skewness of graph and outliers
  mutate(log_wkly_sales = log(weekly_sales)) %>% 
  ggplot(aes(x = log_wkly_sales, fill = type)) +
  geom_histogram() +
  facet_wrap(~ type) +
  # Set the limits of the x-axis
  xlim(0,13) +
  labs(title = "After applying a log, observe a left-skewed distribution for all store types",
       x = "Weekly Sales (logarithmic)",
       y = "Count") +
  theme(legend.position = "none")

```

Of course, we would like to determine whether we have any correlations between the weekly sales and another predictive variable. When looking at the correlation coefficients, we notice that only "size" has a correlation of higher than 0.1 in absolute values, namely 0.240. This also informs us that no variable from the details dataframe seems to be correlated with the weekly sales.

```{r sales_correlation}

joined_sales %>% 
  # Choose random sample from dataframe to make ggpairs run
  sample_n(20000) %>%
  # Select only the numeric values
  select_if(is.numeric) %>% 
  GGally::ggpairs()

```

Intuition tells us that a store would expect more sales during holiday weeks, rather than regular trading weeks. When we proceed to calculate the mean and median for holiday and non-holiday weeks and then subtract them from another, we see that only few stores have a lower mean weekly sales in holiday weeks. When we do the same for the median, we also see only few stores, however it concerns different stores than for the mean. 

```{r mean_median_vs_holidays}

# Calculate mean and median for holiday weeks
mean_med_hol <- joined_sales %>% 
  group_by(store) %>% 
  filter (is_holiday == TRUE) %>% 
  summarize(mean_sales_hol = mean(weekly_sales),
            median_sales_hol = median(weekly_sales))

# Calculate mean and median for normal trading weeks
mean_med_not_hol <- joined_sales %>% 
  group_by(store) %>% 
  filter (is_holiday == FALSE) %>% 
  summarize(mean_sales_not_hol = mean(weekly_sales),
            median_sales_not_hol = median(weekly_sales))

# Joined dataframes above to be able to calculate the difference
joined_mean_med_hol <- mean_med_hol %>% 
  left_join(by = "store", y = mean_med_not_hol) %>%
  mutate(difference_mean = mean_sales_hol - mean_sales_not_hol,
         difference_med = median_sales_hol - median_sales_not_hol)

# Arrange the dataframe to see negative values for mean and median
joined_mean_med_hol %>% 
  arrange(difference_mean)

joined_mean_med_hol %>% 
  arrange(difference_med)

```

```{r spread_by_store}

# Create plot for spread per store
joined_sales %>% 
  group_by(store) %>% 
  mutate(log_wkly_sales = log(weekly_sales)) %>% 
  ggplot(aes(x = log_wkly_sales, y = store)) + 
  geom_boxplot() +
  labs(title = "The medians of log weekly sales for the stores are closely grouped together",
       x = "Weekly Sales (logarithmic)",
       y = "Store")


```

# Inferential Statistics

# Regression
