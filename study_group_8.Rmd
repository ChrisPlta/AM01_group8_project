---
title: "Group8_Project"
author: "Misha Aggarwal, Madalina Dumitrescu, Yung-Chieh Hsu, Wendy Li, Christoph Plachutta, Tianyi Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(skimr)
library(janitor)
library(broom)
library(performance)
library(car)
library(lubridate)
```

```{r set_global_theme}

theme_set(theme_bw())

```

# Exploratory Data Analysis

## Inspecting and Cleaning the Data

As a first step to our EDA, we loud our data into the environment and assign it to variables. We use vroom, as it is superior in speed compared to read.csv.

```{r load_data}

# Load data into environment and assign to variables
sales <- vroom::vroom("data/sales.csv")
details <- vroom::vroom("data/details.csv")
stores <- vroom::vroom("data/stores.csv")

```

Next, we apply janitor::clean_names to bring the column names in order, to also make it easier joinable in the future.

```{r clean_names}

# Clean names of data frames
sales <- janitor::clean_names(sales)
details <- janitor::clean_names(details)
stores <- janitor::clean_names(stores)

```

By inspecting the data, we can see problems that may affect our future analysis. For each data frame, we change types for some of the variables.

```{r inspect_data_sales}

# Inspect Sales data frame
skim(sales)
glimpse(sales)
head(sales)

```
```{r clean_sales_data}

# Change store ID and dept to factor
sales <- sales %>% 
  mutate(store = as.factor(store),
         dept = as.factor(dept),
         # Change date format
         date = dmy(date))

```

```{r inspect_data_details}

# Inspect Sales data frame
skim(details)
glimpse(details)
head(details)

```

```{r clean_details_data}

# Change store ID and dept to factor
details <- details %>% 
  mutate(store = as.factor(store),
         # Change date format
         date = dmy(date),
         # Remove NAs in mark_downs
         mark_down1 = replace_na(mark_down1,0),
         mark_down2 = replace_na(mark_down2,0),
         mark_down3 = replace_na(mark_down3,0),
         mark_down4 = replace_na(mark_down4,0),
         mark_down5 = replace_na(mark_down5,0))

```

```{r inspect_data_stores}

# Inspect Sales data frame
skim(stores)
glimpse(stores)
head(stores)

```
```{r clean_stores_data}

# Change store ID and dept to factor
stores <- stores %>% 
  mutate(store = as.factor(store),
         type = as.factor(type))
```

As a last step, we join the dataframes together into one dataframe. As for the key, we can observe that all data frames share the "store" variable. For sales and details, we also have to include "date" and "is_holiday" in the join.

```{r join_dataframes}

# Join data frames together and assign to variable
joined_sales <- sales %>% 
  left_join(by = c("store","date","is_holiday"), y = details) %>% 
  left_join(by = "store", y = stores)

```

## Exploratory Analysis

We start by looking at whether the dates overlap for sales and details, so that we can assign the details values for each observation in the sales. As we can see from the below code and output, both dataframes start at the same date, while the details spans more weeks then the sales. This allows us to seamlessly join details to the sales.

```{r time_span_diff}

# Calculate weeks between min and max date for both data frames
sales %>% 
  summarize(max_date = max(date),
            min_date = min(date),
            weeks_covered = difftime(max_date, min_date, unit = "weeks"))

details %>% 
  summarize(max_date = max(date),
            min_date = min(date),
            weeks_covered = difftime(max_date, min_date, unit = "weeks"))

```

When investigating the size of the stores, we can also deduct that there are 3 stores with 79 distinct departments, namely 13, 15 and 19. This is the maximum amount of departments within a single store in the sample. Such a high number of departments leads us to make first assumptions about the type of store. We can perhaps the data to come from a company like Walmart, which are known to sell essentially everything in their larger stores, hence a lot of departments.

```{r max_departments}

# Calculate store with maximum departments  
sales %>% 
  group_by(store) %>% 
  summarize(count_distinct = n_distinct(dept)) %>%
  slice_max(count_distinct, n = 1)

```

```{r}

joined_sales %>% 
  group_by(dept) %>% 
  summarize(average_sales = mean(weekly_sales)) %>% 
  arrange


```
To better understand the categorization of stores into their respective types, we investigate their correlated details. By graphing the size of the stores, colored by type, we can see that the size seems to be a significant indicator for a stores categorization. Nevertheless, we observe that some stores are very small, even though they are categorized in, for example, A.

```{r check_store_types}

# Graph size per store type
joined_sales %>% 
  filter(weekly_sales >= 0) %>% 
  mutate(store = fct_reorder(store,size)) %>% 
  ggplot(aes(x = store, y = size, fill = type)) +
        geom_col()

```
When trying to find the store with the most sales in 2011, we isolate store 4, which is indeed categorized as type A. It is also one of the larger stores within

```{r sales_overview_2011}

# Calculate store with most sales in 2011
sales %>% 
  # Select relevant time frame
  filter(date >= "2011-01-07" & date <= "2011-12-30") %>% 
  group_by(store) %>% 
  # Calculate summary statistics
  summarize(annl_sales = sum(weekly_sales),
            avg_wkly_sales = mean(weekly_sales)) %>% 
            slice_max(annl_sales, n = 1)

```

```{r distribution_plot_sales}

joined_sales %>% 
  filter(weekly_sales >= 0) %>% 
  mutate(log_wkly_sales = log(weekly_sales)) %>% 
  ggplot(aes(x = log_wkly_sales)) +
  geom_histogram() +
  facet_wrap(~type)


```

```{r spread_plot_sales}

# Plot distribution of sales
joined_sales %>% 
  group_by(store) %>% 
  ggplot(aes(x = weekly_sales) +
         geom_boxplot() +
         facet_grid()

```
# Inferential Statistics

# Regression
